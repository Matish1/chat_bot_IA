{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bit3ddfad975ce245bbbd61719ef310bbff",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import collections\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "#import pandas\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-8e7dc036c72b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtexto\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"En en En EN vista de la situación que atraviesa el país por cuenta del virus Covid-19 y luego de la solicitud directa del alcalde Daniel Quintero con la intención de proteger la vida la los ciudadanos, los miembros del Consejo Municipal de Gestión del Riesgo decidieron declarar la Calamidad Pública y Emergencia Sanitaria.\"\u001b[0m \u001b[1;31m#53 palabras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text as text\n",
    "texto=\"En en En EN vista de la situación que atraviesa el país por cuenta del virus Covid-19 y luego de la solicitud directa del alcalde Daniel Quintero con la intención de proteger la vida la los ciudadanos, los miembros del Consejo Municipal de Gestión del Riesgo decidieron declarar la Calamidad Pública y Emergencia Sanitaria.\" #53 palabras\n",
    "\n",
    "t=tf.keras.preprocessing.text.Tokenizer(char_level=False)\n",
    "t.fit_on_texts(texto)\n",
    "m=t.texts_to_matrix(texto)\n",
    "s=t.texts_to_sequences(texto)\n",
    "#te=t.texts_to_sequences_generator(texto)\n",
    "print(t.sequences_to_texts([[1], [6]]))\n",
    "print(s)\n",
    "\n",
    "palabras=text.Tokenizer(texto)\n",
    "print(palabras)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertarPalabra(palabra, palabras, posiciones, minuscula):\n",
    "    if len(palabra)==0:\n",
    "        return\n",
    "    if minuscula:\n",
    "        palabra=palabra.lower()\n",
    "    if(palabra not in palabras):\n",
    "        palabras.append(palabra)\n",
    "    indice=palabras.index(palabra)\n",
    "    posiciones.append(indice)\n",
    "def vocabulario(fichero, caracterSeparacion=' ', caracteresPuntuacion=['!','\"','#','$','%','&','(',')','*','+',',','-','.','/',':',';','<','=','>','?','@','[','\\\\','\\'',']','^','_','`','{','|','}','~','\\t','\\n','\\r','¿','¡'],caracteresDescartados=[], minuscula=True, palabras=[]):\n",
    "    posiciones=[]\n",
    "\n",
    "    with open(fichero,mode=\"r\",encoding=\"utf8\") as fichero:\n",
    "        palabra=\"\"\n",
    "        esPuntuacion=False\n",
    "        for linea in fichero:\n",
    "    \n",
    "            for c in linea:\n",
    "                if(c==caracterSeparacion):\n",
    "                    if(esPuntuacion):\n",
    "                        palabra+=c\n",
    "                    insertarPalabra(palabra, palabras, posiciones, minuscula)\n",
    "                    palabra=\"\"\n",
    "                    esPuntuacion=False\n",
    "                else:\n",
    "                    if(c not in caracteresPuntuacion):\n",
    "                        if(esPuntuacion):\n",
    "                            insertarPalabra(palabra, palabras, posiciones,minuscula)\n",
    "                            esPuntuacion=False\n",
    "                            palabra=c\n",
    "                        else:\n",
    "                            palabra+=c\n",
    "                    else:\n",
    "                        insertarPalabra(palabra, palabras, posiciones,minuscula)\n",
    "                        palabra=c\n",
    "                        esPuntuacion=True\n",
    "    return palabras,posiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(posiciones,longitud_bloque):\n",
    "    conjunto=[]\n",
    "    elemento=0\n",
    "    bloque=[]\n",
    "    for i,posicion in enumerate(posiciones):\n",
    "        if elemento==10:\n",
    "            if(len(bloque)>0):\n",
    "                conjunto.append(bloque)\n",
    "                bloque=[]\n",
    "                elemento=0\n",
    "        bloque.append(posicion)\n",
    "        elemento+=1\n",
    "    if(len(bloque)>0):\n",
    "        longitud=len(bloque)\n",
    "        for i in range(longitud_bloque-longitud):\n",
    "            bloque.append(0)\n",
    "        conjunto.append(bloque)\n",
    "    return conjunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Vocabulario= 7460\n"
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-aa6d84dfb5b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vocabulario=\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpalabras\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Entrenamiento=\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mentrenamiento\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test=\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "palabras,posiciones=vocabulario(\"textos\\libro.txt\", palabras=[\"\\n\"])\n",
    "\n",
    "numeroEntrenamiento=math.floor(len(posiciones)*0.8)\n",
    "entrenamiento=posiciones[:numeroEntrenamiento]\n",
    "test=posiciones[numeroEntrenamiento:]\n",
    "\n",
    "longitud_bloque=10\n",
    "entrenamiento=dividir(entrenamiento,longitud_bloque)\n",
    "test=dividir(test,longitud_bloque)\n",
    "\n",
    "\n",
    "print(\"Vocabulario=\",len(palabras))\n",
    "print(\"Entrenamiento=\",entrenamiento.shape)\n",
    "print(\"test=\",len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.00512495,  0.04321748],\n       [ 0.03834225, -0.01898133],\n       [ 0.00431837, -0.03773835],\n       ...,\n       [-0.03060499,  0.01168156],\n       [ 0.0071156 , -0.02809701],\n       [-0.0082266 ,  0.03553892]], dtype=float32)"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embedding_layer = layers.Embedding(len(posiciones), 2)\n",
    "#result = embedding_layer(tf.constant(posiciones))\n",
    "#result.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo(longitud_vocabulario,longitud_embedding=16 ):\n",
    "    model = tf.keras.Sequential([\n",
    "    layers.Embedding(longitud_vocabulario, longitud_embedding),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(longitud_embedding, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-192b3433623f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mentrenamiento\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     validation_data=test, validation_steps=20)\n\u001b[0m",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    593\u001b[0m         use_multiprocessing=use_multiprocessing)\n\u001b[0;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m       (val_x, val_y,\n\u001b[0;32m    597\u001b[0m        \u001b[0mval_sample_weights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack_validation_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "modelo=crearModelo(len(palabras))\n",
    "\n",
    "history = modelo.fit(\n",
    "    entrenamiento,\n",
    "    epochs=10,\n",
    "    validation_data=test, validation_steps=20)\n",
    ""
   ]
  }
 ]
}