{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bit3ddfad975ce245bbbd61719ef310bbff",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "#import pandas\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codificación por Posición#\n",
    "## $PE_{(pos,2i)}=\\sin(pos/10000^{2i/d_{model}})$<br/>$PE_{(pos,2i+1)}=\\cos(pos/10000^{2i/d_{model}})$ ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cogerAngulo(pos, i, d):\n",
    "  ratiosAngulo = 1 / np.power(10000, (2 * (i//2)) / np.float32(d))\n",
    "  return pos * ratiosAngulo\n",
    "\n",
    "def codiciacionPosicional(posicion, d):\n",
    "  angulos = cogerAngulo(np.arange(posicion)[:, np.newaxis], np.arange(d)[np.newaxis, :], d)\n",
    "  \n",
    "  angulos[:, 0::2] = np.sin(angulos[:, 0::2]) #Lo hacemos para los pares\n",
    "  \n",
    "  angulos[:, 1::2] = np.cos(angulos[:, 1::2]) #Lo hacemos para los impares\n",
    "    \n",
    "  posCodificados = angulos[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(posCodificados, dtype=tf.float32)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mascaras ##\n",
    "útiles para despreciar palabras que pueden afectar al comportamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearMascaraEmpaquetada(secuencia):\n",
    "    secuencia=tf.cast(tf.math.equal(secuencia,0), tf.float32)\n",
    "    return secuencia[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def crearMascaraAlFrente(n): #Retorna una máscara diagonal de 0's en el triangulo inferior y de tamaño nxn\n",
    "    mascara=1-tf.linalg.band_part(tf.ones((n,n)),-1,0)\n",
    "    return mascara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATENCIÓN #\n",
    "## $Atencion(Q,K,V)=softmax_k(\\frac{QK^T}{(\\sqrt{d_k})})V$ ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Este método calcula el producto escalar de matrices con la ecuación de Atención\n",
    "def productoEscalarConAtencion(q,k,v, mascaraAtencion):\n",
    "    matrizQK=tf.matmul(q,k,transpose_b=True)\n",
    "    #Calculamos la profuncidad d\n",
    "    dk=tf.cast(tf.shape(k)[-1],tf.float32)\n",
    "    #Valor escalar de atención\n",
    "    atencionEscalar=matrizQK/tf.math.sqrt(dk)\n",
    "\n",
    "    #Sumamos la máscara de atención\n",
    "    if(mascaraAtencion is not None):\n",
    "        atencionEscalar+=(mascaraAtencion*-1e9) #el -1e9 es para generar un valor infinito.\n",
    "    \n",
    "    #Calculamos la función softmax y normalizamos al últmi eje\n",
    "    pesosAtencion=tf.nn.softmax(atencionEscalar, axis=-1)\n",
    "\n",
    "    salida=tf.matmul(pesosAtencion, v)\n",
    "\n",
    "    return salida, pesosAtencion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtencionMultiple(tf.keras.layers.Layer):\n",
    "  def __init__(self, d, numeroCabezas):\n",
    "    super(AtencionMultiple, self).__init__()\n",
    "    self.numeroCabezas = numeroCabezas\n",
    "    self.d = d\n",
    "    \n",
    "    assert d % self.numeroCabezas == 0\n",
    "    \n",
    "    self.profundida = d // self.numeroCabezas #División entera\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d)\n",
    "    self.wk = tf.keras.layers.Dense(d)\n",
    "    self.wv = tf.keras.layers.Dense(d)\n",
    "    \n",
    "    self.capaDensa = tf.keras.layers.Dense(d)\n",
    "\n",
    "  #  Divide la última dimensio dentro (numeroCabezas, profundidad) transponiendo el resultado\n",
    "  # que tiene dimensioines (tamañoPaque, numeroCabezas, longitudSecuencia, profundidad)\n",
    "  def dividirCabezas(self, x, tamañoPaquete):\n",
    "    x = tf.reshape(x, (tamañoPaquete, -1, self.numeroCabezas, self.profundida))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mascara):\n",
    "    tamañoPaquete = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)\n",
    "    k = self.wk(k) \n",
    "    v = self.wv(v)\n",
    "    \n",
    "    q = self.dividirCabezas(q, tamañoPaquete)\n",
    "    k = self.dividirCabezas(k, tamañoPaquete)\n",
    "    v = self.dividirCabezas(v, tamañoPaquete)\n",
    "    \n",
    "    resultadoAtencion, pesosAtencion = productoEscalarConAtencion(q, k, v, mascara)\n",
    "    \n",
    "    resultadoAtencion = tf.transpose(resultadoAtencion, perm=[0, 2, 1, 3])\n",
    "\n",
    "    concatenarAtenciones = tf.reshape(resultadoAtencion, (tamañoPaquete, -1, self.d))\n",
    "\n",
    "    resultado = self.capaDensa(concatenarAtenciones)  \n",
    "        \n",
    "    return resultado, pesosAtencion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redNeuronalHaciaDelante(d, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d)\n",
    "  ])\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapaCodificadora(tf.keras.layers.Layer):\n",
    "  def __init__(self, d, numeroCabezas, dff, ratio=0.1):\n",
    "    super(CapaCodificadora, self).__init__()\n",
    "\n",
    "    self.mCA = AtencionMultiple(d, numeroCabezas)\n",
    "    self.redFF = redNeuronalHaciaDelante(d, dff)\n",
    "\n",
    "    self.capaNormal1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.capaNormal2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.despreciar1 = tf.keras.layers.Dropout(ratio) \n",
    "    self.despreciar2 = tf.keras.layers.Dropout(ratio)\n",
    "    \n",
    "  def call(self, x, entrenamiento, mascara):\n",
    "\n",
    "    salidaAtencion, _ = self.mCA(x, x, x, mascara)\n",
    "    salidaAtencion = self.despreciar1(salidaAtencion, training=entrenamiento)\n",
    "    salida1 = self.capaNormal1(x + salidaAtencion)  \n",
    "    \n",
    "    salidaFFN = self.redFF(salida1)\n",
    "    salidaFFN = self.despreciar2(salidaFFN, training=entrenamiento)\n",
    "    salida2 = self.capaNormal2(salida1 + salidaFFN)  \n",
    "    \n",
    "    return salida2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapaDecodificadora(tf.keras.layers.Layer):\n",
    "    def __init__(self, d, numeroCabeceras, dff, ratio=0.1):\n",
    "        super(CapaDecodificadora, self).__init__()\n",
    "\n",
    "        self.mCA1=AtencionMultiple(d, numeroCabeceras)\n",
    "        self.mCA2=AtencionMultiple(d, numeroCabeceras)\n",
    "\n",
    "        self.redFF=redNeuronalHaciaDelante(d,dff)\n",
    "\n",
    "        self.capaNormal1=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.capaNormal2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.capaNormal3=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.despreciar1=tf.keras.layers.Dropout(ratio)\n",
    "        self.despreciar2=tf.keras.layers.Dropout(ratio)\n",
    "        self.despreciar3=tf.keras.layers.Dropout(ratio)\n",
    "\n",
    "    def call(self,x,salidaCodificada, entrenamiento, mascaraAlFrente, mascaraEmpaquetada):\n",
    "        atencion1, bloquePesosAtencion1 = self.mCA1(x,x,x,mascaraAlFrente)\n",
    "        atencion1=self.despreciar1(atencion1,training=entrenamiento)\n",
    "        salida1=self.capaNormal1(atencion1+x)\n",
    "        #print(salida1)\n",
    "        atencion2, bloquePesosAtencion2 = self.mCA2(salidaCodificada,salidaCodificada,salida1,mascaraEmpaquetada)\n",
    "        atencion2=self.despreciar2(atencion2,training=entrenamiento)\n",
    "        salida2=self.capaNormal2(atencion1+salida1)\n",
    "\n",
    "        salidaRedFF=self.redFF(salida2)\n",
    "        salidaRedFF=self.despreciar3(salidaRedFF, training=entrenamiento)\n",
    "        salida3=self.capaNormal3(salidaRedFF+salida2)\n",
    "\n",
    "        return salida3, bloquePesosAtencion1, bloquePesosAtencion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Codificador(tf.keras.layers.Layer):\n",
    "    def __init__(self, numeroCapas, d, numeroCabeceras, dff, tamañoVocabulario, poisicionMaximaCodificacion, ratio=0.1):\n",
    "        super(Codificador, self).__init__()\n",
    "\n",
    "        self.d=d\n",
    "        self.numeroCapas=numeroCapas\n",
    "\n",
    "        self.embedding=tf.keras.layers.Embedding(tamañoVocabulario, d)\n",
    "        self.posicionesCodificadas=codiciacionPosicional(poisicionMaximaCodificacion, self.d)\n",
    "\n",
    "        self.capasCodificacion=[CapaCodificadora(self.d, numeroCabeceras,dff, ratio) for _ in range(self.numeroCapas)]\n",
    "\n",
    "        self.despreciar=tf.keras.layers.Dropout(ratio)\n",
    "\n",
    "    def call(self, x, entrenamiento, mascara):\n",
    "\n",
    "        longitudSecuencia=tf.shape(x)[1]\n",
    "\n",
    "        x=self.embedding(x)\n",
    "        x*=tf.math.sqrt(tf.cast(self.d,tf.float32))\n",
    "        x+=self.posicionesCodificadas[:, :longitudSecuencia, :]\n",
    "\n",
    "        x=self.despreciar(x, training=entrenamiento)\n",
    "\n",
    "        for i in range(self.numeroCapas):\n",
    "            x=self.capasCodificacion[i](x,entrenamiento,mascara)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decodificador(tf.keras.layers.Layer):\n",
    "    def __init__(self, numeroCapas, d, numeroCabeceras, dff, tamañoVocabulario, posicionMaximaCodificacion, ratio=0.1):\n",
    "        super(Decodificador, self).__init__()\n",
    "\n",
    "        self.d=d\n",
    "        self.numeroCapas=numeroCapas\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(tamañoVocabulario, d)\n",
    "        self.posicionesCodificadas=codiciacionPosicional(posicionMaximaCodificacion, self.d)\n",
    "\n",
    "        self.capasDecodificacion=[CapaDecodificadora(self.d, numeroCabeceras,dff, ratio) for _ in range(self.numeroCapas)]\n",
    "\n",
    "        self.despreciar=tf.keras.layers.Dropout(ratio)\n",
    "\n",
    "    def call(self, x, salidaCodificada, entrenamiento, mascaraAlFrente, mascaraEmpaquetada):\n",
    "\n",
    "        longitudSecuencia=tf.shape(x)[1]\n",
    "        pesosAtencion={}\n",
    "\n",
    "        x=self.embedding(x)\n",
    "        x*=tf.math.sqrt(tf.cast(self.d,tf.float32))\n",
    "        x+=self.posicionesCodificadas[:, :longitudSecuencia, :]\n",
    "\n",
    "        x=self.despreciar(x, training=entrenamiento)\n",
    "\n",
    "        for i in range(self.numeroCapas):\n",
    "            x, bloque1, bloque2 =self.capasDecodificacion[i](x,salidaCodificada,entrenamiento,mascaraAlFrente, mascaraEmpaquetada)\n",
    "\n",
    "            pesosAtencion['capaDecodificadora{}_bloque1'.format(i+1)]=bloque1\n",
    "            pesosAtencion['capaDecodificadora{}_bloque2'.format(i+1)]=bloque2\n",
    "\n",
    "        return x, pesosAtencion\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(64, 62, 512)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "#ejemplo = CapaCodificadora(512, 8, 2048)\n",
    "\n",
    "#salida = ejemplo(\n",
    "#    tf.random.uniform((64, 43, 512)), False, None)\n",
    "#print(salida.shape)\n",
    "#ejemplo2 = CapaDecodificadora(512, 8, 2048)\n",
    "\n",
    "#salida2,_,_ = ejemplo2(tf.random.uniform((64, 50, 512)), salida, False, None, None)\n",
    "\n",
    "#salida2.shape\n",
    "\n",
    "ejemplo=Codificador(2,512,8,2048,8500,10000)\n",
    "entradaTemporal=tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "salida=ejemplo(entradaTemporal, False, None)\n",
    "print(salida.shape)\n",
    "\n",
    "ejemplo2=Decodificador(2,512,8,2048,8000,5000)\n",
    "entradaTemporal=tf.random.uniform((64,26),dtype=tf.int64, minval=0, maxval=200)\n",
    "salida2, atencion=ejemplo2(entradaTemporal, salida,False,None,None)\n",
    "\n",
    "salida2.shape, atencion['capaDecodificadora2_bloque2'].shape\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, numeroCapas, d, numeroCabeceras, dff, tamañoVocabularioEntrada, \n",
    "               tamañoVocabularioSalida, posicionesEntrada, posicionesSalida, ratio=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.codificador = Codificador(numeroCapas, d, numeroCabeceras, dff, \n",
    "                           tamañoVocabularioEntrada, posicionesEntrada, ratio)\n",
    "\n",
    "    self.decodificador = Decodificador(numeroCapas, d, numeroCabeceras, dff, \n",
    "                           tamañoVocabularioSalida, posicionesSalida, ratio)\n",
    "\n",
    "    self.capaFinal = tf.keras.layers.Dense(tamañoVocabularioSalida)\n",
    "    \n",
    "  def call(self, entrada, objetivo, entrenamiento, mascaraCodificadaEmpaquetada, \n",
    "           mascaraAlFrente, mascaraDecodificadaEmpaquetada):\n",
    "\n",
    "    salidaCodificada = self.codificador(entrada, entrenamiento, mascaraCodificadaEmpaquetada)  \n",
    "    \n",
    "  \n",
    "    salidaDecodificada, pesosAtencion = self.decodificador(\n",
    "        objetivo, salidaCodificada, entrenamiento, mascaraAlFrente, mascaraDecodificadaEmpaquetada)\n",
    "    \n",
    "    salida = self.capaFinal(salidaDecodificada) \n",
    "    \n",
    "    return salida, pesosAtencion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TensorShape([64, 36, 8000])"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "ejemplo = Transformer(2, 512, 8, 2048, 8500, 8000, 10000, 6000)\n",
    "\n",
    "entradaTemporal = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "objetivoTemporal = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "salida, _ = ejemplo(entradaTemporal, objetivoTemporal, False, None, None, None)\n",
    "\n",
    "salida.shape  # (batch_size, tar_seq_len, target_vocab_size)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiperparámetros #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos los elementos de converasción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertarPalabra(palabra, palabras, posiciones, minuscula, palabrasInvalidas=[\"\\n\"]):\n",
    "    if len(palabra)==0 or palabra in palabrasInvalidas:\n",
    "        return\n",
    "    if minuscula:\n",
    "        palabra=palabra.lower()\n",
    "    if(palabra not in palabras):\n",
    "        palabras.append(palabra)\n",
    "    indice=palabras.index(palabra)\n",
    "    posiciones.append(indice)\n",
    "#Vocabulario obtiene tanto la lista de palabras como las posiciones por párrafo de cada palabra en el texto dado\n",
    "#juntarParrafos tiene en cuenta que un \\n que no está precedido por un \".\" o un \". \" no se considera como parrafo nuevo\n",
    "def vocabulario(linea, caracterSeparacion=' ', caracteresPuntuacion=['!','\"','#','$','%','&','(',')','*','+',',','-','.','/',':',';','<','=','>','?','@','[','\\\\','\\'',']','^','_','`','{','|','}','~','\\t','\\n','\\r','¿','¡'], caracteresDescartados=['\\r', '\\t'], minuscula=True, palabras=['\\n'], conParrafos=False, juntarParrafos=False, caracteresFinalesParrafo=['.','_']):\n",
    "    posiciones=[]\n",
    "    parrafo=[]\n",
    "    esPuntuacion=False\n",
    "    palabra=\"\"\n",
    "    ultimoCaracter=\"\"\n",
    "    palabrasInvalidas=caracteresDescartados\n",
    "    for c in linea:\n",
    "        if(c==caracterSeparacion):\n",
    "            #if(esPuntuacion):\n",
    "            #    palabra+=c\n",
    "            insertarPalabra(palabra, palabras, parrafo, minuscula,palabrasInvalidas)\n",
    "            palabra=\"\"\n",
    "            esPuntuacion=False\n",
    "        else:\n",
    "            if(c not in caracteresPuntuacion):\n",
    "                if(esPuntuacion):\n",
    "                    insertarPalabra(palabra, palabras, parrafo,minuscula,palabrasInvalidas)\n",
    "                    esPuntuacion=False\n",
    "                    palabra=c\n",
    "                else:\n",
    "                    palabra+=c\n",
    "            else:\n",
    "                insertarPalabra(palabra, palabras, parrafo,minuscula,palabrasInvalidas)\n",
    "                palabra=\"\"\n",
    "                if c not in caracteresDescartados:\n",
    "                    palabra=c\n",
    "                    esPuntuacion=True\n",
    "        if c!=caracterSeparacion and c not in caracteresDescartados and c!='\\n':\n",
    "            ultimoCaracter=c\n",
    "    \n",
    "    if conParrafos and len(parrafo)>0:\n",
    "        if(not juntarParrafos or ultimoCaracter in caracteresFinalesParrafo):\n",
    "            posiciones.append(parrafo)\n",
    "            parrafo=[]\n",
    "    #TODO Implementar un mecanismo para evitar palabras cortadas al final de línea con el -\n",
    "    if palabra!=\"\":\n",
    "        insertarPalabra(palabra, palabras, parrafo,minuscula,palabrasInvalidas)\n",
    "        palabra=\"\"\n",
    "\n",
    "    if(len(parrafo)>0):\n",
    "        posiciones.append(parrafo)\n",
    "        parrafo=[]\n",
    "    return palabras,posiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cogerDiccionario(diccionario,conversaciones):\n",
    "    posicionesConversaciones=[]\n",
    "    for conversacion in conversaciones:\n",
    "        posicionesFrase=[]\n",
    "        for frase in conversacion:\n",
    "            diccionario,posiciones=vocabulario(frase.lower(),palabras=diccionario)\n",
    "            posicionesFrase.append(posiciones)\n",
    "        posicionesConversaciones.append(posicionesFrase)\n",
    "    return diccionario,posicionesConversaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar(posiciones,longitud, tamaño):\n",
    "    #datos=[[longitud]+posiciones+[longitud+1]+[0]*(tamaño-len(posiciones)-2)]\n",
    "    datos=[posiciones+[0]*(tamaño-len(posiciones))]\n",
    "    #datos=posiciones+([0]*(tamaño-len(posiciones)))\n",
    "    #if(datos.shape[0]<tamaño):\n",
    "    #datos.set_shape([None])\n",
    "    return datos\n",
    "#normalizar(posiciones[0][0][0],40)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genera una salida con frases texto - respuesta\n",
    "def transformarConversacion(fichero):\n",
    "    textoAEntrenar=\"\"\n",
    "    conversaciones=[]\n",
    "    conversacion=[]\n",
    "    with open(fichero,mode=\"r\",encoding=\"utf8\") as fichero:\n",
    "        for linea in fichero:\n",
    "            if(linea[0]=='-'):\n",
    "                #Empezamos conversación\n",
    "                if(len(conversacion)>0):\n",
    "                    conversaciones.append(conversacion.copy())\n",
    "                    conversacion.clear()\n",
    "                linea=linea[1:]\n",
    "            frase=linea.strip()\n",
    "            if(len(frase)==0):\n",
    "                continue\n",
    "            if(frase[0]=='-'):\n",
    "                frase=frase[1:].strip()\n",
    "                conversacion.append(frase.strip('\"'))\n",
    "            else:\n",
    "                conversacion[len(conversacion)-1]+=\" \"+frase.strip()\n",
    "    if(len(conversacion)>0):\n",
    "        conversaciones.append(conversacion.copy())\n",
    "    return conversaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Buenos días, ¿cómo estás?', 'Estoy bien, ¿y tú?', 'Yo también estoy bien.', 'Que bueno.', 'Así, es.'] [[[1, 2, 3, 4, 5, 6, 7]], [[8, 9, 3, 4, 10, 11, 7]], [[12, 13, 8, 9, 14]], [[15, 16, 14]], [[17, 3, 18, 14]]]\n1657 \n\n"
    }
   ],
   "source": [
    "ficheros=[\"textos/dialogo.txt\"]\n",
    "diccionario=['\\n']\n",
    "for fichero in ficheros:\n",
    "    conversaciones=transformarConversacion(fichero)\n",
    "    diccionario,posiciones=cogerDiccionario(diccionario,conversaciones)\n",
    "    print(conversaciones[0],posiciones[0])\n",
    "    print(len(diccionario),diccionario[0])\n",
    "\n",
    "tamañoVocabularioEntrada = len(diccionario) + 2\n",
    "tamañoVocabularioSalida = len(diccionario) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[<tf.Tensor: shape=(1, 48), dtype=int64, numpy=\narray([[1, 2, 3, 4, 5, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0]], dtype=int64)>, <tf.Tensor: shape=(1, 48), dtype=int64, numpy=\narray([[ 8,  9,  3,  4, 10, 11,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n      dtype=int64)>]\n"
    }
   ],
   "source": [
    "datosEntrenamiento=[]\n",
    "for conversacion in posiciones:\n",
    "    if(len(conversacion)>1):\n",
    "        for i in range(len(conversacion)-1):\n",
    "            datosEntrenamiento.append([tf.constant(normalizar(conversacion[i][0],tamañoVocabularioEntrada-2,48),tf.int64),tf.constant(normalizar(conversacion[i+1][0],tamañoVocabularioEntrada-2,48),tf.int64)])\n",
    "\n",
    "print(datosEntrenamiento[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCAS=20\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "condicionesEntrenamiento = [\n",
    "    tf.TensorSpec(shape=(48, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(48, None), dtype=tf.int64),\n",
    "]\n",
    "#@tf.function(input_signature=condicionesEntrenamiento)\n",
    "def pasoEntrenamiento(entrada, objetivo):\n",
    "  #entradaObjetivo = objetivo[:, :-1]\n",
    "  #objetivoReal = objetivo[:, 1:]\n",
    "  entradaObjetivo = objetivo\n",
    "  objetivoReal = objetivo\n",
    "  \n",
    "  mascaraCodificadaEmpaquetada, mascaraCombinada, mascaraDecodificadaEmpaquetada = crearMascara(entrada,entradaObjetivo)\n",
    "  #print(mascaraCodificadaEmpaquetada, mascaraCombinada, mascaraDecodificadaEmpaquetada)\n",
    "  \n",
    "  with tf.GradientTape() as cadenaEntrada:\n",
    "    cadenaEntrada.watch(transformador.trainable_variables)\n",
    "    prediccion, _ = transformador(entrada, entradaObjetivo, \n",
    "                                 True, \n",
    "                                 mascaraCodificadaEmpaquetada, \n",
    "                                 mascaraCombinada, \n",
    "                                 mascaraDecodificadaEmpaquetada)\n",
    "    perdidas = funcionPerdida(objetivoReal, prediccion)\n",
    "  #print(transformador.trainable_variables)\n",
    "  gradiente = cadenaEntrada.gradient(perdidas, transformador.trainable_variables)  \n",
    "  #print(list(zip(gradiente, transformador.trainable_variables))[1])  \n",
    "  optimizador.apply_gradients(zip(gradiente, transformador.trainable_variables))\n",
    "  \n",
    "  entrenamientoPerdidos(perdidas)\n",
    "  entrenamientoPrecision(objetivoReal,prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "invalid action: 'warning'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-89bef691c254>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'warning'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpasoEntrenamiento\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatosEntrenamiento\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatosEntrenamiento\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python37\\lib\\warnings.py\u001b[0m in \u001b[0;36mfilterwarnings\u001b[1;34m(action, message, category, module, lineno, append)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m    142\u001b[0m     assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n\u001b[1;32m--> 143\u001b[1;33m                       \"once\"), \"invalid action: %r\" % (action,)\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"message must be a string\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"category must be a class\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: invalid action: 'warning'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pasoEntrenamiento(datosEntrenamiento[0][0], datosEntrenamiento[0][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar datos de entrenamiento ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeroCapas = 4 #6\n",
    "d = 128 #512\n",
    "dff = 512 #2048\n",
    "numeroCabeceras = 8\n",
    "tamañoVocabularioEntrada = len(diccionario) + 2\n",
    "tamañoVocabularioSalida = len(diccionario) + 2\n",
    "ratioDespreciar = 0.1\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizador #\n",
    "## $lrate=d^{-0.5}_{model}*\\min(step\\_num^{-0.5},step\\_num*warmup\\_steps^{-1.5})$ ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgramarOptimizacion(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d, pasosWarmup=4000):\n",
    "    super(ProgramarOptimizacion, self).__init__()\n",
    "    \n",
    "    self.d = d\n",
    "    self.d = tf.cast(self.d, tf.float32)\n",
    "\n",
    "    self.pasosWarmup = pasosWarmup\n",
    "    \n",
    "  def __call__(self, paso):\n",
    "    argumento1 = tf.math.rsqrt(paso)\n",
    "    argumento2 = paso * (self.pasosWarmup ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d) * tf.math.minimum(argumento1, argumento2)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Text(0.5, 0, 'Train Step')"
     },
     "metadata": {},
     "execution_count": 24
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 405.564427 262.19625\" width=\"405.564427pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 405.564427 262.19625 \r\nL 405.564427 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 62.86875 224.64 \r\nL 397.66875 224.64 \r\nL 397.66875 7.2 \r\nL 62.86875 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m0f47b370c1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"78.086932\" xlink:href=\"#m0f47b370c1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(74.905682 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"116.133338\" xlink:href=\"#m0f47b370c1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5000 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(103.408338 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"154.179743\" xlink:href=\"#m0f47b370c1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10000 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(138.273493 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"192.226149\" xlink:href=\"#m0f47b370c1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15000 -->\r\n      <g transform=\"translate(176.319899 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"230.272555\" xlink:href=\"#m0f47b370c1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20000 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(214.366305 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.31896\" xlink:href=\"#m0f47b370c1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25000 -->\r\n      <g transform=\"translate(252.41271 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"306.365366\" xlink:href=\"#m0f47b370c1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 30000 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(290.459116 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"344.411772\" xlink:href=\"#m0f47b370c1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 35000 -->\r\n      <g transform=\"translate(328.505522 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"382.458177\" xlink:href=\"#m0f47b370c1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 40000 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(366.551927 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_10\">\r\n     <!-- Train Step -->\r\n     <defs>\r\n      <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n      <path d=\"M 53.515625 70.515625 \r\nL 53.515625 60.890625 \r\nQ 47.90625 63.578125 42.921875 64.890625 \r\nQ 37.9375 66.21875 33.296875 66.21875 \r\nQ 25.25 66.21875 20.875 63.09375 \r\nQ 16.5 59.96875 16.5 54.203125 \r\nQ 16.5 49.359375 19.40625 46.890625 \r\nQ 22.3125 44.4375 30.421875 42.921875 \r\nL 36.375 41.703125 \r\nQ 47.40625 39.59375 52.65625 34.296875 \r\nQ 57.90625 29 57.90625 20.125 \r\nQ 57.90625 9.515625 50.796875 4.046875 \r\nQ 43.703125 -1.421875 29.984375 -1.421875 \r\nQ 24.8125 -1.421875 18.96875 -0.25 \r\nQ 13.140625 0.921875 6.890625 3.21875 \r\nL 6.890625 13.375 \r\nQ 12.890625 10.015625 18.65625 8.296875 \r\nQ 24.421875 6.59375 29.984375 6.59375 \r\nQ 38.421875 6.59375 43.015625 9.90625 \r\nQ 47.609375 13.234375 47.609375 19.390625 \r\nQ 47.609375 24.75 44.3125 27.78125 \r\nQ 41.015625 30.8125 33.5 32.328125 \r\nL 27.484375 33.5 \r\nQ 16.453125 35.6875 11.515625 40.375 \r\nQ 6.59375 45.0625 6.59375 53.421875 \r\nQ 6.59375 63.09375 13.40625 68.65625 \r\nQ 20.21875 74.21875 32.171875 74.21875 \r\nQ 37.3125 74.21875 42.625 73.28125 \r\nQ 47.953125 72.359375 53.515625 70.515625 \r\nz\r\n\" id=\"DejaVuSans-83\"/>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n     </defs>\r\n     <g transform=\"translate(204.574219 252.916562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-84\"/>\r\n      <use x=\"60.865234\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"101.978516\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"163.257812\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"191.041016\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"254.419922\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"286.207031\" xlink:href=\"#DejaVuSans-83\"/>\r\n      <use x=\"349.683594\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"388.892578\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"450.416016\" xlink:href=\"#DejaVuSans-112\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_10\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m62e9e8aea2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#m62e9e8aea2\" y=\"214.756364\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.0000 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 218.555582)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#m62e9e8aea2\" y=\"186.467744\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.0002 -->\r\n      <g transform=\"translate(20.878125 190.266963)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#m62e9e8aea2\" y=\"158.179125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.0004 -->\r\n      <g transform=\"translate(20.878125 161.978344)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#m62e9e8aea2\" y=\"129.890506\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.0006 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 133.689725)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#m62e9e8aea2\" y=\"101.601887\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 0.0008 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 105.401105)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#m62e9e8aea2\" y=\"73.313267\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 0.0010 -->\r\n      <g transform=\"translate(20.878125 77.112486)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#m62e9e8aea2\" y=\"45.024648\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- 0.0012 -->\r\n      <g transform=\"translate(20.878125 48.823867)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_17\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#m62e9e8aea2\" y=\"16.736029\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_18\">\r\n      <!-- 0.0014 -->\r\n      <g transform=\"translate(20.878125 20.535248)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_19\">\r\n     <!-- Learning Rate -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 8.296875 \r\nL 55.171875 8.296875 \r\nL 55.171875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-76\"/>\r\n      <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n      <path d=\"M 44.390625 34.1875 \r\nQ 47.5625 33.109375 50.5625 29.59375 \r\nQ 53.5625 26.078125 56.59375 19.921875 \r\nL 66.609375 0 \r\nL 56 0 \r\nL 46.6875 18.703125 \r\nQ 43.0625 26.03125 39.671875 28.421875 \r\nQ 36.28125 30.8125 30.421875 30.8125 \r\nL 19.671875 30.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nL 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.578125 72.90625 50.734375 67.671875 \r\nQ 56.890625 62.453125 56.890625 51.90625 \r\nQ 56.890625 45.015625 53.6875 40.46875 \r\nQ 50.484375 35.9375 44.390625 34.1875 \r\nz\r\nM 19.671875 64.796875 \r\nL 19.671875 38.921875 \r\nL 32.078125 38.921875 \r\nQ 39.203125 38.921875 42.84375 42.21875 \r\nQ 46.484375 45.515625 46.484375 51.90625 \r\nQ 46.484375 58.296875 42.84375 61.546875 \r\nQ 39.203125 64.796875 32.078125 64.796875 \r\nz\r\n\" id=\"DejaVuSans-82\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 150.96375)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-76\"/>\r\n      <use x=\"55.697266\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"117.220703\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"178.5\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"219.597656\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"282.976562\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"310.759766\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"374.138672\" xlink:href=\"#DejaVuSans-103\"/>\r\n      <use x=\"437.615234\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"469.402344\" xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"538.853516\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"600.132812\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"639.341797\" xlink:href=\"#DejaVuSans-101\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#pd00749c0f7)\" d=\"M 78.086932 214.756364 \r\nL 108.524056 17.083636 \r\nL 108.59254 17.305633 \r\nL 110.822059 24.148172 \r\nL 113.158109 30.60565 \r\nL 115.585469 36.665602 \r\nL 118.111751 42.377543 \r\nL 120.736953 47.766945 \r\nL 123.453466 52.84372 \r\nL 126.322165 57.732238 \r\nL 129.297394 62.361892 \r\nL 132.424808 66.812394 \r\nL 135.666362 71.037129 \r\nL 139.029664 75.059199 \r\nL 142.545152 78.922066 \r\nL 146.296528 82.710184 \r\nL 150.222917 86.354125 \r\nL 154.354757 89.880679 \r\nL 158.646391 93.252443 \r\nL 163.280443 96.603205 \r\nL 168.0819 99.798295 \r\nL 173.202946 102.935779 \r\nL 178.628364 105.994645 \r\nL 184.335324 108.955949 \r\nL 190.605372 111.946044 \r\nL 197.324367 114.884699 \r\nL 204.043363 117.584982 \r\nL 211.546114 120.355856 \r\nL 219.688045 123.110007 \r\nL 228.172393 125.73807 \r\nL 237.265484 128.318051 \r\nL 246.959708 130.835733 \r\nL 257.217019 133.273875 \r\nL 268.562457 135.737827 \r\nL 280.448154 138.093523 \r\nL 294.395967 140.606347 \r\nL 308.944912 142.98088 \r\nL 323.89715 145.198121 \r\nL 340.150574 147.389678 \r\nL 358.557425 149.637767 \r\nL 377.687158 151.751003 \r\nL 382.450568 152.24598 \r\nL 382.450568 152.24598 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 62.86875 224.64 \r\nL 62.86875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 397.66875 224.64 \r\nL 397.66875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 62.86875 224.64 \r\nL 397.66875 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 62.86875 7.2 \r\nL 397.66875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pd00749c0f7\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"62.86875\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "ratioAprendizaje=ProgramarOptimizacion(d)\n",
    "optimizador=tf.keras.optimizers.Adam(ratioAprendizaje, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "ratioAprendizajeProgramadoTemporal=ProgramarOptimizacion(d)\n",
    "\n",
    "plt.plot(ratioAprendizajeProgramadoTemporal(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")\n",
    "\n",
    "#Text(0.5, 0, 'Train Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "objetoPerdido = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def funcionPerdida(real, predicho):\n",
    "  mascara = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  perdido = objetoPerdido(real, predicho)\n",
    "\n",
    "  mascara = tf.cast(mascara, dtype=perdido.dtype)\n",
    "  perdido *= mascara\n",
    "  \n",
    "  return tf.reduce_mean(perdido)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrenamientoPerdidos = tf.keras.metrics.Mean(name='entrenamientoPerdidos')\n",
    "entrenamientoPrecision = tf.keras.metrics.SparseCategoricalAccuracy(name='entrenamientoPrecision')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento y puntos de control #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformador = Transformer(numeroCapas, d, numeroCabeceras, dff,\n",
    "                          tamañoVocabularioEntrada, tamañoVocabularioSalida, \n",
    "                          tamañoVocabularioEntrada, tamañoVocabularioSalida,\n",
    "                          ratioDespreciar)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearMascara(entrada, objetivo):\n",
    "  # Encoder padding mask\n",
    "  mascaraCodificadaEmpaquetada = crearMascaraEmpaquetada(entrada)\n",
    "  mascaraDecodificadaEmpaquetada = crearMascaraEmpaquetada(entrada)\n",
    "  mascaraAlFrente = crearMascaraAlFrente(tf.shape(objetivo)[1])\n",
    "  mascaraDecodificadaEmpaqutadaObjetivo = crearMascaraEmpaquetada(entrada)\n",
    "  \n",
    "  mascarasCombinadas = tf.maximum(mascaraDecodificadaEmpaqutadaObjetivo, mascaraAlFrente)\n",
    "  return mascaraCodificadaEmpaquetada, mascarasCombinadas, mascaraDecodificadaEmpaquetada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(\n[[1 2 3 4 5 6 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 48), dtype=int64)\ntf.Tensor(\n[[ 8  9  3  4 10 11  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]], shape=(1, 48), dtype=int64)\ntf.Tensor(\n[[ 8  9  3  4 10 11  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]], shape=(1, 48), dtype=int64)\ntf.Tensor(\n[[[[0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n    1. 1. 1. 1.]]]], shape=(1, 1, 1, 48), dtype=float32)\ntf.Tensor(\n[[[[0. 1. 1. ... 1. 1. 1.]\n   [0. 0. 1. ... 1. 1. 1.]\n   [0. 0. 0. ... 1. 1. 1.]\n   ...\n   [0. 0. 0. ... 1. 1. 1.]\n   [0. 0. 0. ... 1. 1. 1.]\n   [0. 0. 0. ... 1. 1. 1.]]]], shape=(1, 1, 48, 48), dtype=float32)\ntf.Tensor(\n[[[[0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n    1. 1. 1. 1.]]]], shape=(1, 1, 1, 48), dtype=float32)\n"
    }
   ],
   "source": [
    "print(datosEntrenamiento[0][0])\n",
    "#entradaObjetivo = datosEntrenamiento[0][1][:, :-1]\n",
    "entradaObjetivo = datosEntrenamiento[0][1]\n",
    "print(entradaObjetivo)\n",
    "#objetivoReal = datosEntrenamiento[0][1][:, 1:]\n",
    "objetivoReal = datosEntrenamiento[0][1]\n",
    "print(objetivoReal)\n",
    "mascaraCodificadaEmpaquetada, mascaraCombinada, mascaraDecodificadaEmpaquetada = crearMascara(datosEntrenamiento[0][0],entradaObjetivo)\n",
    "print(mascaraCodificadaEmpaquetada)\n",
    "print(mascaraCombinada)\n",
    "print(mascaraDecodificadaEmpaquetada)\n",
    "prediccion, _ = transformador(datosEntrenamiento[0][0], entradaObjetivo, \n",
    "                                 True, \n",
    "                                 mascaraCodificadaEmpaquetada, \n",
    "                                 mascaraCombinada, \n",
    "                                 mascaraDecodificadaEmpaquetada)\n",
    "#pasoEntrenamiento(datosEntrenamiento[0][0], datosEntrenamiento[0][1])\n",
    "#datosEntrenamiento[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ficheroPuntoControl=\"./modelos/seq2seq\"\n",
    "puntoControl=tf.train.Checkpoint(transformer=transformador, optimizer=optimizador)\n",
    "\n",
    "manejarPuntoControl=tf.train.CheckpointManager(puntoControl, ficheroPuntoControl, max_to_keep=5)\n",
    "\n",
    "if manejarPuntoControl.latest_checkpoint:\n",
    "  puntoControl.restore(manejarPuntoControl.latest_checkpoint)\n",
    "  print ('Restauramos el último punto de control')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ansformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_95/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_95/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_96/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_96/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_103/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_103/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_104/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_104/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_105/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_105/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_106/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_106/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_113/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_113/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_114/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_114/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_115/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_115/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_116/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_116/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_123/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_123/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_124/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_124/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_125/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_125/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_126/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_126/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['transformer_1/codificador_2/embedding_4/embeddings:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_65/kernel:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_65/bias:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_66/kernel:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_66/bias:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_67/kernel:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_67/bias:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_68/kernel:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_68/bias:0', 'transformer_1/codificador_2/capa_codificadora_4/sequential_8/dense_69/kernel:0', 'transformer_1/codificador_2/capa_codificadora_4/sequential_8/dense_69/bias:0', 'transformer_1/codificador_2/capa_codificadora_4/sequential_8/dense_70/kernel:0', 'transformer_1/codificador_2/capa_codificadora_4/sequential_8/dense_70/bias:0', 'transformer_1/codificador_2/capa_codificadora_4/layer_normalization_20/gamma:0', 'transformer_1/codificador_2/capa_codificadora_4/layer_normalization_20/beta:0', 'transformer_1/codificador_2/capa_codificadora_4/layer_normalization_21/gamma:0', 'transformer_1/codificador_2/capa_codificadora_4/layer_normalization_21/beta:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_71/kernel:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_71/bias:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_72/kernel:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_72/bias:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_73/kernel:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_73/bias:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_74/kernel:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_74/bias:0', 'transformer_1/codificador_2/capa_codificadora_5/sequential_9/dense_75/kernel:0', 'transformer_1/codificador_2/capa_codificadora_5/sequential_9/dense_75/bias:0', 'transformer_1/codificador_2/capa_codificadora_5/sequential_9/dense_76/kernel:0', 'transformer_1/codificador_2/capa_codificadora_5/sequential_9/dense_76/bias:0', 'transformer_1/codificador_2/capa_codificadora_5/layer_normalization_22/gamma:0', 'transformer_1/codificador_2/capa_codificadora_5/layer_normalization_22/beta:0', 'transformer_1/codificador_2/capa_codificadora_5/layer_normalization_23/gamma:0', 'transformer_1/codificador_2/capa_codificadora_5/layer_normalization_23/beta:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_77/kernel:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_77/bias:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_78/kernel:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_78/bias:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_79/kernel:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_79/bias:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_80/kernel:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_80/bias:0', 'transformer_1/codificador_2/capa_codificadora_6/sequential_10/dense_81/kernel:0', 'transformer_1/codificador_2/capa_codificadora_6/sequential_10/dense_81/bias:0', 'transformer_1/codificador_2/capa_codificadora_6/sequential_10/dense_82/kernel:0', 'transformer_1/codificador_2/capa_codificadora_6/sequential_10/dense_82/bias:0', 'transformer_1/codificador_2/capa_codificadora_6/layer_normalization_24/gamma:0', 'transformer_1/codificador_2/capa_codificadora_6/layer_normalization_24/beta:0', 'transformer_1/codificador_2/capa_codificadora_6/layer_normalization_25/gamma:0', 'transformer_1/codificador_2/capa_codificadora_6/layer_normalization_25/beta:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_83/kernel:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_83/bias:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_84/kernel:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_84/bias:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_85/kernel:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_85/bias:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_86/kernel:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_86/bias:0', 'transformer_1/codificador_2/capa_codificadora_7/sequential_11/dense_87/kernel:0', 'transformer_1/codificador_2/capa_codificadora_7/sequential_11/dense_87/bias:0', 'transformer_1/codificador_2/capa_codificadora_7/sequential_11/dense_88/kernel:0', 'transformer_1/codificador_2/capa_codificadora_7/sequential_11/dense_88/bias:0', 'transformer_1/codificador_2/capa_codificadora_7/layer_normalization_26/gamma:0', 'transformer_1/codificador_2/capa_codificadora_7/layer_normalization_26/beta:0', 'transformer_1/codificador_2/capa_codificadora_7/layer_normalization_27/gamma:0', 'transformer_1/codificador_2/capa_codificadora_7/layer_normalization_27/beta:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_93/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_93/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_94/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_94/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_95/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_95/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_96/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_96/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_103/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_103/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_104/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_104/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_105/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_105/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_106/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_106/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_113/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_113/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_114/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_114/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_115/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_115/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_116/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_116/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_123/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_123/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_124/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_124/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_125/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_125/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_126/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_126/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['transformer_1/codificador_2/embedding_4/embeddings:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_65/kernel:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_65/bias:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_66/kernel:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_66/bias:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_67/kernel:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_67/bias:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_68/kernel:0', 'transformer_1/codificador_2/capa_codificadora_4/atencion_multiple_12/dense_68/bias:0', 'transformer_1/codificador_2/capa_codificadora_4/sequential_8/dense_69/kernel:0', 'transformer_1/codificador_2/capa_codificadora_4/sequential_8/dense_69/bias:0', 'transformer_1/codificador_2/capa_codificadora_4/sequential_8/dense_70/kernel:0', 'transformer_1/codificador_2/capa_codificadora_4/sequential_8/dense_70/bias:0', 'transformer_1/codificador_2/capa_codificadora_4/layer_normalization_20/gamma:0', 'transformer_1/codificador_2/capa_codificadora_4/layer_normalization_20/beta:0', 'transformer_1/codificador_2/capa_codificadora_4/layer_normalization_21/gamma:0', 'transformer_1/codificador_2/capa_codificadora_4/layer_normalization_21/beta:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_71/kernel:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_71/bias:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_72/kernel:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_72/bias:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_73/kernel:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_73/bias:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_74/kernel:0', 'transformer_1/codificador_2/capa_codificadora_5/atencion_multiple_13/dense_74/bias:0', 'transformer_1/codificador_2/capa_codificadora_5/sequential_9/dense_75/kernel:0', 'transformer_1/codificador_2/capa_codificadora_5/sequential_9/dense_75/bias:0', 'transformer_1/codificador_2/capa_codificadora_5/sequential_9/dense_76/kernel:0', 'transformer_1/codificador_2/capa_codificadora_5/sequential_9/dense_76/bias:0', 'transformer_1/codificador_2/capa_codificadora_5/layer_normalization_22/gamma:0', 'transformer_1/codificador_2/capa_codificadora_5/layer_normalization_22/beta:0', 'transformer_1/codificador_2/capa_codificadora_5/layer_normalization_23/gamma:0', 'transformer_1/codificador_2/capa_codificadora_5/layer_normalization_23/beta:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_77/kernel:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_77/bias:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_78/kernel:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_78/bias:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_79/kernel:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_79/bias:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_80/kernel:0', 'transformer_1/codificador_2/capa_codificadora_6/atencion_multiple_14/dense_80/bias:0', 'transformer_1/codificador_2/capa_codificadora_6/sequential_10/dense_81/kernel:0', 'transformer_1/codificador_2/capa_codificadora_6/sequential_10/dense_81/bias:0', 'transformer_1/codificador_2/capa_codificadora_6/sequential_10/dense_82/kernel:0', 'transformer_1/codificador_2/capa_codificadora_6/sequential_10/dense_82/bias:0', 'transformer_1/codificador_2/capa_codificadora_6/layer_normalization_24/gamma:0', 'transformer_1/codificador_2/capa_codificadora_6/layer_normalization_24/beta:0', 'transformer_1/codificador_2/capa_codificadora_6/layer_normalization_25/gamma:0', 'transformer_1/codificador_2/capa_codificadora_6/layer_normalization_25/beta:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_83/kernel:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_83/bias:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_84/kernel:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_84/bias:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_85/kernel:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_85/bias:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_86/kernel:0', 'transformer_1/codificador_2/capa_codificadora_7/atencion_multiple_15/dense_86/bias:0', 'transformer_1/codificador_2/capa_codificadora_7/sequential_11/dense_87/kernel:0', 'transformer_1/codificador_2/capa_codificadora_7/sequential_11/dense_87/bias:0', 'transformer_1/codificador_2/capa_codificadora_7/sequential_11/dense_88/kernel:0', 'transformer_1/codificador_2/capa_codificadora_7/sequential_11/dense_88/bias:0', 'transformer_1/codificador_2/capa_codificadora_7/layer_normalization_26/gamma:0', 'transformer_1/codificador_2/capa_codificadora_7/layer_normalization_26/beta:0', 'transformer_1/codificador_2/capa_codificadora_7/layer_normalization_27/gamma:0', 'transformer_1/codificador_2/capa_codificadora_7/layer_normalization_27/beta:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_93/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_93/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_94/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_94/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_95/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_95/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_96/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_4/atencion_multiple_17/dense_96/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_103/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_103/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_104/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_104/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_105/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_105/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_106/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_5/atencion_multiple_19/dense_106/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_113/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_113/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_114/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_114/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_115/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_115/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_116/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_6/atencion_multiple_21/dense_116/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_123/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_123/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_124/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_124/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_125/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_125/bias:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_126/kernel:0', 'transformer_1/decodificador_2/capa_decodificadora_7/atencion_multiple_23/dense_126/bias:0'] when minimizing the loss.\nEpoca 1 Lote 700 Perdida 1.3570 Precisión 0.0176\nEpoca 1 Perdida 1.3570 Precisión 0.0176\nTiempo que tomo desde la primera cada epoca: 165.70453643798828 secs\n\n"
    }
   ],
   "source": [
    "EPOCAS=1\n",
    "for epoca in range(EPOCAS):\n",
    "  tiempoInicial = time.time()\n",
    "  \n",
    "  entrenamientoPerdidos.reset_states()\n",
    "  entrenamientoPrecision.reset_states()\n",
    "  for (i, datos) in enumerate(datosEntrenamiento):\n",
    "    pasoEntrenamiento(datos[0], datos[1])\n",
    "    if i % 50 == 0:\n",
    "      print ('Epoca {} Lote {} Perdida {:.4f} Precisión {:.4f}'.format(\n",
    "          epoca + 1, i, entrenamientoPerdidos.result(), entrenamientoPrecision.result()))\n",
    "      \n",
    "  if (epoca + 1) % 5 == 0:\n",
    "    puntoControlGuardado = manejarPuntoControl.save()\n",
    "    print ('Salvando punto de control en la epoca {} a {}'.format(epoca+1,\n",
    "                                                         puntoControlGuardado))\n",
    "    \n",
    "  print ('Epoca {} Perdida {:.4f} Precisión {:.4f}'.format(epoca + 1, \n",
    "                                                entrenamientoPerdidos.result(), entrenamientoPrecision.result()))\n",
    "\n",
    "  print ('Tiempo que tomo desde la primera cada epoca: {} secs\\n'.format(time.time() - tiempoInicial))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<SubwordTextEncoder vocab_size=8087>"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']\n",
    "\n",
    "\n",
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "tokenizer_en\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<SubwordTextEncoder vocab_size=8087>"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "tokenizer_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7915 ----> T\n1248 ----> ran\n7946 ----> s\n7194 ----> former \n13 ----> is \n2799 ----> awesome\n7877 ----> .\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "in converted code:\n\n\n    TypeError: tf__tf_enc() takes 1 positional argument but 2 were given\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-04746fb23701>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mresultado_l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNONE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresultado_l\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrain_examples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_enc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[0;32m   2302\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2303\u001b[0m       return DatasetV1Adapter(\n\u001b[1;32m-> 2304\u001b[1;33m           MapDataset(self, map_func, preserve_cardinality=False))\n\u001b[0m\u001b[0;32m   2305\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2306\u001b[0m       return DatasetV1Adapter(\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   3886\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3887\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3888\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   3889\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[0;32m   3890\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3146\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2393\u001b[0m     \u001b[1;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[1;32m-> 2395\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2396\u001b[0m     \u001b[1;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2397\u001b[0m     \u001b[1;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                           converted_func)\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3138\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3139\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3140\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3141\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3080\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: in converted code:\n\n\n    TypeError: tf__tf_enc() takes 1 positional argument but 2 were given\n"
     ]
    }
   ],
   "source": [
    "#for ts in tokenized_string:\n",
    "#    print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))\n",
    "\n",
    "def enc(l):\n",
    "    l = [tokenizer_en.vocab_size] + tokenizer_en.encode(l) + [tokenizer_en.vocab_size+1]\n",
    "    return l\n",
    "def tf_enc(l):\n",
    "    resultado_l=tf.py_function(enc,[en], [tf.int64])\n",
    "    resultado_l.set_shape([NONE])\n",
    "    return resultado_l\n",
    "train_examples.map(tf_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(<tf.Tensor: shape=(), dtype=string, numpy=b'e quando melhoramos a procura , tiramos a \\xc3\\xbanica vantagem da impress\\xc3\\xa3o , que \\xc3\\xa9 a serendipidade .'>, <tf.Tensor: shape=(), dtype=string, numpy=b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .'>)\n"
    }
   ],
   "source": [
    "for i,dato in enumerate(train_examples):\n",
    "    if(i==0):\n",
    "        print(dato)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'map'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-1f249e71a52e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadded_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#m.set_shape([None])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'map'"
     ]
    }
   ],
   "source": []
  }
 ]
}